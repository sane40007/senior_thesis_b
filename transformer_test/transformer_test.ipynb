{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install janome\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import vocab\n",
    "import torchtext.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"./JEC_basic_sentence_v1-3.xlsx\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#日本語用のトークン変換関数を作成\n",
    "j_t = Tokenizer()\n",
    "def j_tokenizer(text): \n",
    "    return [tok for tok in j_t.tokenize(text, wakati=True)]\n",
    "\n",
    "#英語用のトークン変換関数を作成\n",
    "e_t = spacy.load('en_core_web_sm')\n",
    "def e_tokenizer(text):\n",
    "    return [tok.text for tok in e_t.tokenizer(text)]\n",
    "\n",
    "#各文章をトークンに変換\n",
    "texts = df.iloc[:,0].apply(j_tokenizer)\n",
    "targets = df.iloc[:,1].apply(e_tokenizer)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#日本語のトークン数（単語数）をカウント\n",
    "j_list = []\n",
    "for i in range(len(texts)):\n",
    "  j_list.extend(texts[i])\n",
    "j_counter = Counter()\n",
    "j_counter.update(j_list)\n",
    "j_v = vocab(j_counter, specials=(['<unk>', '<pad>', '<bos>', '<eos>']))   #特殊文字の定義\n",
    "j_v.set_default_index(j_v['<unk>'])\n",
    "\n",
    "#英語のトークン数（単語数）をカウント\n",
    "e_list = []\n",
    "for i in range(len(targets)):\n",
    "  e_list.extend(targets[i])\n",
    "e_counter = Counter()\n",
    "e_counter.update(e_list)\n",
    "e_v = vocab(e_counter, specials=(['<unk>', '<pad>', '<bos>', '<eos>']))   #特殊文字の定義\n",
    "e_v.set_default_index(e_v['<unk>'])\n",
    "\n",
    "enc_vocab_size, dec_vocab_size = len(j_v), len(e_v)\n",
    "print(enc_vocab_size, dec_vocab_size)   #6446 6072\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各言語ごとに単語数を合わせる必要がある為、1文当たりの単語数を14に指定\n",
    "j_word_count = 14\n",
    "e_word_count = 14\n",
    "\n",
    "j_text_transform = T.Sequential(\n",
    "  T.VocabTransform(j_v),   #トークンに変換\n",
    "  T.Truncate(j_word_count),   #14語以上の文章を14語で切る\n",
    "  T.AddToken(token=j_v['<bos>'], begin=True),   #文頭に'<bos>\n",
    "  T.AddToken(token=j_v['<eos>'], begin=False),   #文末に'<eos>'を追加\n",
    "  T.ToTensor(),   #テンソルに変換\n",
    "  T.PadTransform(j_word_count + 2, j_v['<pad>'])   #14語に満たない文章を'<pad>'で埋めて14語に合わせる\n",
    ")\n",
    "\n",
    "e_text_transform = T.Sequential(\n",
    "  T.VocabTransform(e_v),   #トークンに変換\n",
    "  T.Truncate(e_word_count),   #14語以上の文章を14語で切る\n",
    "  T.AddToken(token=e_v['<bos>'], begin=True),   #文頭に'<bos>\n",
    "  T.AddToken(token=e_v['<eos>'], begin=False),   #文末に'<eos>'を追加\n",
    "  T.ToTensor(),   #テンソルに変換\n",
    "  T.PadTransform(e_word_count + 2, e_v['<pad>'])   #14語に満たない文章を'<pad>'で埋めて14語に合わせる\n",
    ")\n",
    "\n",
    "class Dataset(Dataset):\n",
    "  def __init__(\n",
    "      self,\n",
    "      df,\n",
    "      j_text_transform,\n",
    "      e_text_transform,\n",
    "      ):\n",
    "    \n",
    "    self.texts = df.iloc[:,0].apply(j_tokenizer)\n",
    "    self.targets = df.iloc[:,1].apply(e_tokenizer)\n",
    "    self.j_text_transform = j_text_transform\n",
    "    self.e_text_transform = e_text_transform\n",
    "  \n",
    "  def max_word(self):\n",
    "    return len(self.j_v), len(self.e_v)\n",
    "        \n",
    "  def __getitem__(self, i):\n",
    "    text = self.texts[i]\n",
    "    text = self.j_text_transform([text]).squeeze()\n",
    "\n",
    "    target = self.targets[i]\n",
    "    target = self.e_text_transform([target]).squeeze()\n",
    "\n",
    "    dec_input = target[:-1]\n",
    "    dec_target = target[1:]   #右に1つずらす\n",
    "    data = {\"text\": text, \"dec_input\": dec_input, \"dec_target\": dec_target}\n",
    "    return data\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "dataset = Dataset(df, j_text_transform, e_text_transform)\n",
    "data_loader = DataLoader(dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4,\n",
    "                          drop_last=True,\n",
    "                          shuffle=True)\n",
    "\n",
    "data = next(iter(data_loader))\n",
    "text, dec_input, target = data[\"text\"], data[\"dec_input\"], data[\"dec_target\"]\n",
    "print(text[0], dec_input[0], target[0], sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, dropout = 0.1, max_len = 5000):\n",
    "    super().__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "    position = torch.arange(max_len).unsqueeze(1).to(device)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim)).to(device)\n",
    "    pe = torch.zeros(max_len, 1, dim).to(device)\n",
    "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:x.size(0)]\n",
    "    return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  \n",
    "  def __init__(self, dim, head_num, dropout = 0.1):\n",
    "    super().__init__() \n",
    "    self.dim = dim\n",
    "    self.head_num = head_num\n",
    "    self.linear_Q = nn.Linear(dim, dim, bias = False)\n",
    "    self.linear_K = nn.Linear(dim, dim, bias = False)\n",
    "    self.linear_V = nn.Linear(dim, dim, bias = False)\n",
    "    self.linear = nn.Linear(dim, dim, bias = False)\n",
    "    self.soft = nn.Softmax(dim = 3)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def split_head(self, x):\n",
    "    x = torch.tensor_split(x, self.head_num, dim = 2)\n",
    "    x = torch.stack(x, dim = 1)\n",
    "    return x\n",
    "  \n",
    "  def concat_head(self, x):\n",
    "    x = torch.tensor_split(x, x.size()[1], dim = 1)\n",
    "    x = torch.concat(x, dim = 3).squeeze(dim = 1)\n",
    "    return x\n",
    "\n",
    "  def forward(self, Q, K, V, mask = None):\n",
    "    Q = self.linear_Q(Q)   #(BATCH_SIZE,word_count,dim)\n",
    "    K = self.linear_K(K)\n",
    "    V = self.linear_V(V)\n",
    "    \n",
    "    Q = self.split_head(Q)   #(BATCH_SIZE,head_num,word_count//head_num,dim)\n",
    "    K = self.split_head(K)\n",
    "    V = self.split_head(V)\n",
    "\n",
    "    QK = torch.matmul(Q, torch.transpose(K, 3, 2))\n",
    "    QK = QK/((self.dim//self.head_num)**0.5)\n",
    "\n",
    "    if mask is not None:\n",
    "      QK = QK + mask\n",
    "\n",
    "    softmax_QK = self.soft(QK)\n",
    "    softmax_QK = self.dropout(softmax_QK)\n",
    "\n",
    "    QKV = torch.matmul(softmax_QK, V)\n",
    "    QKV = self.concat_head(QKV)\n",
    "    QKV = self.linear(QKV)\n",
    "    return QKV\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, hidden_dim = 2048, dropout = 0.1):\n",
    "    super().__init__() \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.linear_1 = nn.Linear(dim, hidden_dim)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.linear_2 = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear_1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.linear_2(x)\n",
    "    return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, head_num, dropout = 0.1):\n",
    "    super().__init__() \n",
    "    self.MHA = MultiHeadAttention(dim, head_num)\n",
    "    self.layer_norm_1 = nn.LayerNorm([dim])\n",
    "    self.layer_norm_2 = nn.LayerNorm([dim])\n",
    "    self.FF = FeedForward(dim)\n",
    "    self.dropout_1 = nn.Dropout(dropout)\n",
    "    self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    Q = K = V = x\n",
    "    x = self.MHA(Q, K, V)\n",
    "    x = self.dropout_1(x)\n",
    "    x = x + Q\n",
    "    x = self.layer_norm_1(x)\n",
    "    _x = x\n",
    "    x = self.FF(x)\n",
    "    x = self.dropout_2(x)\n",
    "    x = x + _x\n",
    "    x = self.layer_norm_2(x)\n",
    "    return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(self, enc_vocab_size, dim, head_num, dropout = 0.1):\n",
    "    super().__init__() \n",
    "    self.dim = dim\n",
    "    self.embed = nn.Embedding(enc_vocab_size, dim)\n",
    "    self.PE = PositionalEncoding(dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.EncoderBlocks = nn.ModuleList([EncoderBlock(dim, head_num) for _ in range(6)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embed(x)\n",
    "    x = x*(self.dim**0.5)\n",
    "    x = self.PE(x)\n",
    "    x = self.dropout(x)\n",
    "    for i in range(6):\n",
    "      x = self.EncoderBlocks[i](x)\n",
    "    return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, head_num, dropout = 0.1):\n",
    "    super().__init__() \n",
    "    self.MMHA = MultiHeadAttention(dim, head_num)\n",
    "    self.MHA = MultiHeadAttention(dim, head_num)\n",
    "    self.layer_norm_1 = nn.LayerNorm([dim])\n",
    "    self.layer_norm_2 = nn.LayerNorm([dim])\n",
    "    self.layer_norm_3 = nn.LayerNorm([dim])\n",
    "    self.FF = FeedForward(dim)\n",
    "    self.dropout_1 = nn.Dropout(dropout)\n",
    "    self.dropout_2 = nn.Dropout(dropout)\n",
    "    self.dropout_3 = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, y, mask):\n",
    "    Q = K = V = x\n",
    "    x = self.MMHA(Q, K, V, mask)\n",
    "    x = self.dropout_1(x)\n",
    "    x = x + Q\n",
    "    x = self.layer_norm_1(x)\n",
    "    Q = x\n",
    "    K = V = y\n",
    "    x = self.MHA(Q, K, V)\n",
    "    x = self.dropout_2(x)\n",
    "    x = x + Q\n",
    "    x = self.layer_norm_2(x)\n",
    "    _x = x\n",
    "    x = self.FF(x)\n",
    "    x = self.dropout_3(x)\n",
    "    x = x + _x\n",
    "    x = self.layer_norm_3(x)\n",
    "    return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(self, dec_vocab_size, dim, head_num, dropout = 0.1):\n",
    "    super().__init__() \n",
    "    self.dim = dim\n",
    "    self.embed = nn.Embedding(dec_vocab_size, dim)\n",
    "    self.PE = PositionalEncoding(dim)\n",
    "    self.DecoderBlocks = nn.ModuleList([DecoderBlock(dim, head_num) for _ in range(6)])\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.linear = nn.Linear(dim, dec_vocab_size)\n",
    "\n",
    "  def forward(self, x, y, mask):\n",
    "    x = self.embed(x)\n",
    "    x = x*(self.dim**0.5)\n",
    "    x = self.PE(x)\n",
    "    x = self.dropout(x)\n",
    "    for i in range(6):\n",
    "      x = self.DecoderBlocks[i](x, y, mask)\n",
    "    x = self.linear(x)   #損失の計算にnn.CrossEntropyLoss()を使用する為、Softmax層を挿入しない\n",
    "    return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  \n",
    "  def __init__(self, enc_vocab_size, dec_vocab_size, dim, head_num):\n",
    "    super().__init__() \n",
    "    self.encoder = Encoder(enc_vocab_size, dim, head_num)\n",
    "    self.decoder = Decoder(dec_vocab_size, dim, head_num)\n",
    "\n",
    "  def forward(self, enc_input, dec_input, mask):\n",
    "    enc_output = self.encoder(enc_input)\n",
    "    output = self.decoder(dec_input, enc_output, mask)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(enc_vocab_size, dec_vocab_size, dim = 512, e_word_count + 2, j_word_count + 1, head_num = 8).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epoch_num = 200\n",
    "print_coef = 10\n",
    "train_length = len(dataset)\n",
    "\n",
    "history = {\"train_loss\": []}\n",
    "n = 0\n",
    "train_loss = 0\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "  model.train()\n",
    "  for i, data in enumerate(data_loader):\n",
    "    optimizer.zero_grad()\n",
    "    text, dec_input, target = data[\"text\"].to(device), data[\"dec_input\"].to(device), data[\"dec_target\"].to(device)\n",
    "    mask = nn.Transformer.generate_square_subsequent_mask(j_word_count + 1).to(device)   #マスクの作成\n",
    "\n",
    "    outputs = model(text, dec_input, mask)\n",
    "    target = nn.functional.one_hot(target, dec_vocab_size).to(torch.float32)\n",
    "\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "    history[\"train_loss\"].append(loss.item())\n",
    "    n += 1\n",
    "    if i % ((train_length//BATCH_SIZE)//print_coef) == (train_length//BATCH_SIZE)//print_coef - 1:\n",
    "      print(f\"epoch:{epoch+1}  index:{i+1}  loss:{train_loss/n:.10f}\")\n",
    "      train_loss = 0\n",
    "      n = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history[\"train_loss\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
